{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg16\n",
    "from collections import namedtuple\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "from torchvision import utils as vutils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.gn = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gn(x)\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block = nn.Sequential(\n",
    "            GroupNorm(in_channels),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            GroupNorm(out_channels),\n",
    "            Swish(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.channel_up = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.in_channels != self.out_channels:\n",
    "            return self.channel_up(x) + self.block(x)\n",
    "        else:\n",
    "            return x + self.block(x)\n",
    "\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(UpSampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2.0)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DownSampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 2, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pad = (0, 1, 0, 1)\n",
    "        x = F.pad(x, pad, mode=\"constant\", value=0)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "        self.in_channels = channels\n",
    "\n",
    "        self.gn = GroupNorm(channels)\n",
    "        self.q = nn.Conv2d(channels, channels, 1, 1, 0)\n",
    "        self.k = nn.Conv2d(channels, channels, 1, 1, 0)\n",
    "        self.v = nn.Conv2d(channels, channels, 1, 1, 0)\n",
    "        self.proj_out = nn.Conv2d(channels, channels, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = self.gn(x)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        b, c, h, w = q.shape\n",
    "\n",
    "        q = q.reshape(b, c, h*w)\n",
    "        q = q.permute(0, 2, 1)\n",
    "        k = k.reshape(b, c, h*w)\n",
    "        v = v.reshape(b, c, h*w)\n",
    "\n",
    "        attn = torch.bmm(q, k)\n",
    "        attn = attn * (int(c)**(-0.5))\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        attn = attn.permute(0, 2, 1)\n",
    "\n",
    "        A = torch.bmm(v, attn)\n",
    "        A = A.reshape(b, c, h, w)\n",
    "\n",
    "        return x + A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        channels = [128, 128, 128, 256, 256, 512]\n",
    "        attn_resolutions = [16]\n",
    "        num_res_blocks = 2\n",
    "        resolution = 256\n",
    "        layers = [nn.Conv2d(args.image_channels, channels[0], 3, 1, 1)]\n",
    "        for i in range(len(channels)-1):\n",
    "            in_channels = channels[i]\n",
    "            out_channels = channels[i + 1]\n",
    "            for j in range(num_res_blocks):\n",
    "                layers.append(ResidualBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "                if resolution in attn_resolutions:\n",
    "                    layers.append(NonLocalBlock(in_channels))\n",
    "            if i != len(channels)-2:\n",
    "                layers.append(DownSampleBlock(channels[i+1]))\n",
    "                resolution //= 2\n",
    "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
    "        layers.append(NonLocalBlock(channels[-1]))\n",
    "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
    "        layers.append(GroupNorm(channels[-1]))\n",
    "        layers.append(Swish())\n",
    "        layers.append(nn.Conv2d(channels[-1], args.latent_dim, 3, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Decoder, self).__init__()\n",
    "        channels = [512, 256, 256, 128, 128]\n",
    "        attn_resolutions = [16]\n",
    "        num_res_blocks = 3\n",
    "        resolution = 16\n",
    "\n",
    "        in_channels = channels[0]\n",
    "        layers = [nn.Conv2d(args.latent_dim, in_channels, 3, 1, 1),\n",
    "                  ResidualBlock(in_channels, in_channels),\n",
    "                  NonLocalBlock(in_channels),\n",
    "                  ResidualBlock(in_channels, in_channels)]\n",
    "\n",
    "        for i in range(len(channels)):\n",
    "            out_channels = channels[i]\n",
    "            for j in range(num_res_blocks):\n",
    "                layers.append(ResidualBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "                if resolution in attn_resolutions:\n",
    "                    layers.append(NonLocalBlock(in_channels))\n",
    "            if i != 0:\n",
    "                layers.append(UpSampleBlock(in_channels))\n",
    "                resolution *= 2\n",
    "\n",
    "        layers.append(GroupNorm(in_channels))\n",
    "        layers.append(Swish())\n",
    "        layers.append(nn.Conv2d(in_channels, args.image_channels, 3, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Codebook(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Codebook, self).__init__()\n",
    "        self.num_codebook_vectors = args.num_codebook_vectors\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.beta = args.beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_codebook_vectors, self.latent_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.num_codebook_vectors, 1.0 / self.num_codebook_vectors)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened = z.view(-1, self.latent_dim)\n",
    "\n",
    "        d = torch.sum(z_flattened**2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - \\\n",
    "            2*(torch.matmul(z_flattened, self.embedding.weight.t()))\n",
    "\n",
    "        min_encoding_indices = torch.argmin(d, dim=1)\n",
    "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
    "\n",
    "        loss = torch.mean((z_q.detach() - z)**2) + self.beta * torch.mean((z_q - z.detach())**2)\n",
    "\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        z_q = z_q.permute(0, 3, 1, 2)\n",
    "\n",
    "        return z_q, min_encoding_indices, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQGAN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VQGAN, self).__init__()\n",
    "        self.encoder = Encoder(args).to(device=args.device)\n",
    "        self.decoder = Decoder(args).to(device=args.device)\n",
    "        self.codebook = Codebook(args).to(device=args.device)\n",
    "        self.quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
    "        self.post_quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        encoded_images = self.encoder(imgs)\n",
    "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
    "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
    "        post_quant_conv_mapping = self.post_quant_conv(codebook_mapping)\n",
    "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
    "\n",
    "        return decoded_images, codebook_indices, q_loss\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        encoded_images = self.encoder(imgs)\n",
    "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
    "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
    "        return codebook_mapping, codebook_indices, q_loss\n",
    "\n",
    "    def decode(self, z):\n",
    "        post_quant_conv_mapping = self.post_quant_conv(z)\n",
    "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
    "        return decoded_images\n",
    "\n",
    "    def calculate_lambda(self, perceptual_loss, gan_loss):\n",
    "        last_layer = self.decoder.model[-1]\n",
    "        last_layer_weight = last_layer.weight\n",
    "        perceptual_loss_grads = torch.autograd.grad(perceptual_loss, last_layer_weight, retain_graph=True)[0]\n",
    "        gan_loss_grads = torch.autograd.grad(gan_loss, last_layer_weight, retain_graph=True)[0]\n",
    "\n",
    "        λ = torch.norm(perceptual_loss_grads) / (torch.norm(gan_loss_grads) + 1e-4)\n",
    "        λ = torch.clamp(λ, 0, 1e4).detach()\n",
    "        return 0.8 * λ\n",
    "\n",
    "    @staticmethod\n",
    "    def adopt_weight(disc_factor, i, threshold, value=0.):\n",
    "        if i < threshold:\n",
    "            disc_factor = value\n",
    "        return disc_factor\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrminiator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args, num_filters_last=64, n_layers=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(args.image_channels, num_filters_last, 4, 2, 1), nn.LeakyReLU(0.2)]\n",
    "        num_filters_mult = 1\n",
    "\n",
    "        for i in range(1, n_layers + 1):\n",
    "            num_filters_mult_last = num_filters_mult\n",
    "            num_filters_mult = min(2 ** i, 8)\n",
    "            layers += [\n",
    "                nn.Conv2d(num_filters_last * num_filters_mult_last, num_filters_last * num_filters_mult, 4,\n",
    "                          2 if i < n_layers else 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPIPS \n",
    "### (perceptual loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_MAP = {\n",
    "    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n",
    "}\n",
    "\n",
    "CKPT_MAP = {\n",
    "    \"vgg_lpips\": \"vgg.pth\"\n",
    "}\n",
    "\n",
    "\n",
    "def download(url, local_path, chunk_size=1024):\n",
    "    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        total_size = int(r.headers.get(\"content-length\", 0))\n",
    "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                for data in r.iter_content(chunk_size=chunk_size):\n",
    "                    if data:\n",
    "                        f.write(data)\n",
    "                        pbar.update(chunk_size)\n",
    "\n",
    "\n",
    "def get_ckpt_path(name, root):\n",
    "    assert name in URL_MAP\n",
    "    path = os.path.join(root, CKPT_MAP[name])\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {name} model from {URL_MAP[name]} to {path}\")\n",
    "        download(URL_MAP[name], path)\n",
    "    return path\n",
    "\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LPIPS, self).__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        self.channels = [64, 128, 256, 512, 512]\n",
    "        self.vgg = VGG16()\n",
    "        self.lins = nn.ModuleList([\n",
    "            NetLinLayer(self.channels[0]),\n",
    "            NetLinLayer(self.channels[1]),\n",
    "            NetLinLayer(self.channels[2]),\n",
    "            NetLinLayer(self.channels[3]),\n",
    "            NetLinLayer(self.channels[4])\n",
    "        ])\n",
    "\n",
    "        self.load_from_pretrained()\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def load_from_pretrained(self, name=\"vgg_lpips\"):\n",
    "        ckpt = get_ckpt_path(name, \"vgg_lpips\")\n",
    "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "\n",
    "    def forward(self, real_x, fake_x):\n",
    "        features_real = self.vgg(self.scaling_layer(real_x))\n",
    "        features_fake = self.vgg(self.scaling_layer(fake_x))\n",
    "        diffs = {}\n",
    "\n",
    "        for i in range(len(self.channels)):\n",
    "            diffs[i] = (norm_tensor(features_real[i]) - norm_tensor(features_fake[i])) ** 2\n",
    "\n",
    "        return sum([spatial_average(self.lins[i].model(diffs[i])) for i in range(len(self.channels))])\n",
    "\n",
    "\n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        self.register_buffer(\"shift\", torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer(\"scale\", torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.shift) / self.scale\n",
    "\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=1):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        vgg_pretrained_features = vgg16(pretrained=True).features\n",
    "        slices = [vgg_pretrained_features[i] for i in range(30)]\n",
    "        self.slice1 = nn.Sequential(*slices[0:4])\n",
    "        self.slice2 = nn.Sequential(*slices[4:9])\n",
    "        self.slice3 = nn.Sequential(*slices[9:16])\n",
    "        self.slice4 = nn.Sequential(*slices[16:23])\n",
    "        self.slice5 = nn.Sequential(*slices[23:30])\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.slice1(x)\n",
    "        h_relu1 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5 = h\n",
    "        vgg_outputs = namedtuple(\"VGGOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
    "        return vgg_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n",
    "\n",
    "\n",
    "def norm_tensor(x):\n",
    "    \"\"\"\n",
    "    Normalize images by their length to make them unit vector?\n",
    "    :param x: batch of images\n",
    "    :return: normalized batch of images\n",
    "    \"\"\"\n",
    "    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
    "    return x / (norm_factor + 1e-10)\n",
    "\n",
    "\n",
    "def spatial_average(x):\n",
    "    \"\"\"\n",
    "     imgs have: batch_size x channels x width x height --> average over width and height channel\n",
    "    :param x: batch of images\n",
    "    :return: averaged images along width and height\n",
    "    \"\"\"\n",
    "    return x.mean([2, 3], keepdim=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePaths(Dataset):\n",
    "    def __init__(self, path, size=None):\n",
    "        self.size = size\n",
    "\n",
    "        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n",
    "        self._length = len(self.images)\n",
    "\n",
    "        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n",
    "        self.cropper = albumentations.CenterCrop(height=self.size, width=self.size)\n",
    "        self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = self.preprocessor(image=image)[\"image\"]\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = self.preprocess_image(self.images[i])\n",
    "        return example\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    train_data = ImagePaths(args.dataset_path, size=256)\n",
    "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "# --------------------------------------------- #\n",
    "#                  Module Utils\n",
    "#            for Encoder, Decoder etc.\n",
    "# --------------------------------------------- #\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "def plot_images(images):\n",
    "    x = images[\"input\"]\n",
    "    reconstruction = images[\"rec\"]\n",
    "    half_sample = images[\"half_sample\"]\n",
    "    full_sample = images[\"full_sample\"]\n",
    "\n",
    "    fig, axarr = plt.subplots(1, 4)\n",
    "    axarr[0].imshow(x.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
    "    axarr[1].imshow(reconstruction.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
    "    axarr[2].imshow(half_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
    "    axarr[3].imshow(full_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VQGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainVQGAN:\n",
    "    def __init__(self, args):\n",
    "        self.vqgan = VQGAN(args).to(device=args.device)\n",
    "        self.discriminator = Discriminator(args).to(device=args.device)\n",
    "        self.discriminator.apply(weights_init)\n",
    "        self.perceptual_loss = LPIPS().eval().to(device=args.device)\n",
    "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
    "\n",
    "        self.prepare_training()\n",
    "\n",
    "        self.train(args)\n",
    "\n",
    "    def configure_optimizers(self, args):\n",
    "        lr = args.learning_rate\n",
    "        opt_vq = torch.optim.Adam(\n",
    "            list(self.vqgan.encoder.parameters()) +\n",
    "            list(self.vqgan.decoder.parameters()) +\n",
    "            list(self.vqgan.codebook.parameters()) +\n",
    "            list(self.vqgan.quant_conv.parameters()) +\n",
    "            list(self.vqgan.post_quant_conv.parameters()),\n",
    "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2)\n",
    "        )\n",
    "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
    "                                    lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
    "\n",
    "        return opt_vq, opt_disc\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_training():\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    def train(self, args):\n",
    "        train_dataset = load_data(args)\n",
    "        steps_per_epoch = len(train_dataset)\n",
    "        for epoch in range(args.epochs):\n",
    "            with tqdm(range(len(train_dataset))) as pbar:\n",
    "                for i, imgs in zip(pbar, train_dataset):\n",
    "                    imgs = imgs.to(device=args.device)\n",
    "                    decoded_images, _, q_loss = self.vqgan(imgs)\n",
    "\n",
    "                    disc_real = self.discriminator(imgs)\n",
    "                    disc_fake = self.discriminator(decoded_images)\n",
    "\n",
    "                    disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch*steps_per_epoch+i, threshold=args.disc_start)\n",
    "\n",
    "                    perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
    "                    rec_loss = torch.abs(imgs - decoded_images)\n",
    "                    perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n",
    "                    perceptual_rec_loss = perceptual_rec_loss.mean()\n",
    "                    g_loss = -torch.mean(disc_fake)\n",
    "\n",
    "                    λ = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
    "                    vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n",
    "\n",
    "                    d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
    "                    d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
    "                    gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n",
    "\n",
    "                    self.opt_vq.zero_grad()\n",
    "                    vq_loss.backward(retain_graph=True)\n",
    "\n",
    "                    self.opt_disc.zero_grad()\n",
    "                    gan_loss.backward()\n",
    "\n",
    "                    self.opt_vq.step()\n",
    "                    self.opt_disc.step()\n",
    "\n",
    "                    if i % 10 == 0:\n",
    "                        with torch.no_grad():\n",
    "                            real_fake_images = torch.cat((imgs[:4], decoded_images.add(1).mul(0.5)[:4]))\n",
    "                            vutils.save_image(real_fake_images, os.path.join(\"results\", f\"{epoch}_{i}.jpg\"), nrow=4)\n",
    "\n",
    "                    pbar.set_postfix(\n",
    "                        VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
    "                        GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
    "                    )\n",
    "                    pbar.update(0)\n",
    "                torch.save(self.vqgan.state_dict(), os.path.join(\"checkpoints\", f\"vqgan_epoch_{epoch}.pt\"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
    "    parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
    "    parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
    "    parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
    "    parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
    "    parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
    "    parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
    "    parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
    "    parser.add_argument('--batch-size', type=int, default=6, help='Input batch size for training (default: 6)')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train (default: 50)')\n",
    "    parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
    "    parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
    "    parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
    "    parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator (default: 0)')\n",
    "    parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
    "    parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
    "    parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.dataset_path = r\"C:\\Users\\dome\\datasets\\flowers\"\n",
    "\n",
    "    train_vqgan = TrainVQGAN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c25b4b2c57b09438cc71d238efc717c1af779df49b89ae0e4109cdc4adf8ace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
